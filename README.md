# Control of car using hand gestures

## Table of contents
- [Team Members](#team-members)
- [Project Demo](#project-demo)
- [Description](#description)
- [Technologies](#technologies)
- [Project Video](#project-video)
- [Contributing](#contributing)



## Team Members
- Omar Ahmed
- Karim Ibrahim Yehia
- Eslam Mohamed Abdelbassir
- Ahmed Hesham
- Karim Kamal Rizk
- Ahmed Samir
- Andrew Magdy



## Project Demo
This project we have designed a Hand Gesture Controlled Robot using RasperryPi to classify hand gestures using ensemble of deep learning architectures for decision makin in the control of car robot.This Hand Gesture controlled Robot is based on RasperryPi, WebCam and L293D Motor Driver. 

The 4 poses we designed our system upon are:
•	“1” which indicates turning right 	
•	“5” which indicates turning left
•	“2” which indicates moving forward
•	“0” which indicates stopping and not moving in any way

The low-level control architecture was simple depending on the GPIOs of our controller “Raspberry pi” where we used a motor driver to drive the wheels motors.
So the high-level control algorithm which takes the image and processes it resulting in determining the gesture make use of the low-level functions to drive the car.

## Description

### Usage

The most important scripts used in the implementation of our project are:
 1) collectingdata.py
 2) training.py
 3) Low_level.py
 4) testing.py
 
They are listed in the order of their usage. Firstly, we used "collectingdata.py" for collecting data -images- generated by our team members. These images are pre-processed in several ways similar to the way they are processed in the "testing.py" module, that's in order to extract the hand from its background and this extracted image is then fed to the CNN.

Secondly,You can find are several training scripts that can be used but we finally used the "training.py" and "training2.py" scripts for training the convolutional neural network used for determining and classifying the hand gestures since the other ones required large disk space for saving the model and this wasn't suitable for us. Although, the best results came from using the " inceptionV2" as well as using transfer learning -that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem- and data augmentation -means for making the network robust to certain types of variations. We introduced noise in form of random rotation, flipping, shear, zoom and translation-. 

We then defined the functions that drive the raspberry pi GPIOs in the "Low_levels.py" module which is imported later on by our testing module. Finally, the "testing.py" module is the one which makes use of the saved model, where it loads the model and processes the images captured by the camera for feeding it to the loaded CNN model and then using the low level functions imported, it starts driving the RC car. 


### Platforms required

The main platforms and libraries needed are:
 - OpenCV
 - tensorflow
 - keras
 - matplotlib


## Project Video
[Youtube](https://youtu.be/YxlOvFcngS4)


## Contributing
The preprocessing and collection of data is based on the following tutorial [ Computer Vision and Machine Learning with Python, Keras and OpenCV](https://github.com/jrobchin/Computer-Vision-Basics-with-Python-Keras-and-OpenCV/).


